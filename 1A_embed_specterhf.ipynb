{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to load C version of sampling, use python version instead.\n",
      "Failed to load fast version of SpMM, use torch.scatter_add instead.\n"
     ]
    }
   ],
   "source": [
    "# use sciemb conda env: conda activate sciemb\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from  cogdl import oagbert\n",
    "import torch\n",
    "from datetime import datetime\n",
    "import json\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataEncoder:\n",
    "\n",
    "    def __init__(self, model='specter_hf'):\n",
    "        # Init the tokenizer and encoding the model\n",
    "        if model == 'specter_hf':\n",
    "            self.load_specter_hf()\n",
    "        elif model == 'scibert':\n",
    "            self.load_scibert()\n",
    "        elif model == 'oagbert':\n",
    "            self.load_oagbert()\n",
    "        \n",
    "        # self.model.to('cuda:1')\n",
    "        self.model.eval()\n",
    "        return\n",
    "\n",
    "    def update_encoding_model(self, model_name):\n",
    "        if model_name == 'specter_hf':\n",
    "            self.load_specter()\n",
    "        elif model_name == 'scibert':\n",
    "            self.load_scibert()\n",
    "        elif model_name == 'oagbert':\n",
    "            self.load_oagbert()\n",
    "        else:\n",
    "            print('Invalid model name! Please try again with a valid  model name.')\n",
    "\n",
    "    def load_specter_hf(self):\n",
    "        self.model_name = 'specter_hf'\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained('allenai/specter')\n",
    "        self.model = AutoModel.from_pretrained('allenai/specter')\n",
    "        return\n",
    "\n",
    "    def load_scibert(self):\n",
    "        self.model_name = 'scibert'\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained('allenai/scibert_scivocab_cased')\n",
    "        self.model = AutoModel.from_pretrained('allenai/scibert_scivocab_cased')\n",
    "        return\n",
    "\n",
    "    def load_oagbert(self):\n",
    "        self.model_name = 'oagbert'\n",
    "        self.tokenizer, self.model = oagbert(\"oagbert-v2\")\n",
    "        return\n",
    "\n",
    "    def encode_batch_wise_using_specter(self, document_dict, to_save_loc, batch_size=20):\n",
    "        # List to contain small batch of douments and the corresponding doc ids\n",
    "        doc_batch_list = []\n",
    "        batch_ids = []\n",
    "\n",
    "        document_emb_dict = {}\n",
    "\n",
    "        for _, d in enumerate(document_dict):\n",
    "            if _ % 2000 == 0:\n",
    "                print(\"Encoded: {} @ {}\".format(_, datetime.now().strftime('%Y-%m-%d %H:%M:%S')))\n",
    "\n",
    "            doc_batch_list.append((document_dict[d].get('title') or '') + self.tokenizer.sep_token + (document_dict[d].get('abstract') or ''))\n",
    "            batch_ids.append(d)\n",
    "\n",
    "            if _%batch_size == 0:\n",
    "                inputs = self.tokenizer(doc_batch_list, padding=True, truncation=True, return_tensors=\"pt\", max_length=512)\n",
    "                # inputs = inputs.to('cuda:1')\n",
    "                result = self.model(**inputs)\n",
    "                embeddings = result.last_hidden_state[:, 0, :]\n",
    "\n",
    "                for ii, k in enumerate(batch_ids):\n",
    "                    document_emb_dict[k] = embeddings[ii].detach().cpu().numpy().tolist()\n",
    "\n",
    "                doc_batch_list = []\n",
    "                batch_ids = []\n",
    "\n",
    "        if batch_ids:\n",
    "            inputs = self.tokenizer(doc_batch_list, padding=True, truncation=True, return_tensors=\"pt\", max_length=512)\n",
    "            # inputs = inputs.to('cuda:1')\n",
    "            result = self.model(**inputs)\n",
    "            embeddings = result.last_hidden_state[:, 0, :]\n",
    "\n",
    "            for _, k in enumerate(batch_ids):\n",
    "                document_emb_dict[k] = embeddings[_].detach().cpu().numpy().tolist()\n",
    "\n",
    "        # To freeup memory in case of reuse in jupyter\n",
    "        doc_batch_list = []\n",
    "        batch_ids = []\n",
    "\n",
    "        with open(to_save_loc, 'w') as outfile:\n",
    "            for _, entry in enumerate(document_emb_dict):\n",
    "                try:\n",
    "                    outfile.write(json.dumps({\"paper_id\": entry, \"embedding\": document_emb_dict[entry]}) + '\\n')\n",
    "                except:\n",
    "                    print(_, entry)\n",
    "                    continue\n",
    "        return document_emb_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "de = DataEncoder()\n",
    "\n",
    "all_task_files = glob.glob(\"./data/*.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./data/paper_metadata_mag_mesh.json',\n",
       " './data/paper_metadata_recomm.json',\n",
       " './data/paper_metadata_view_cite_read.json']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_task_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded: 0 @ 2022-06-17 13:19:10\n",
      "Encoded: 2000 @ 2022-06-17 13:36:27\n",
      "Encoded: 4000 @ 2022-06-17 13:54:24\n",
      "Encoded: 6000 @ 2022-06-17 14:12:06\n",
      "Encoded: 8000 @ 2022-06-17 14:29:53\n",
      "Encoded: 10000 @ 2022-06-17 14:47:01\n",
      "Encoded: 12000 @ 2022-06-17 15:04:05\n",
      "Encoded: 14000 @ 2022-06-17 15:21:59\n",
      "Encoded: 16000 @ 2022-06-17 15:39:58\n",
      "Encoded: 18000 @ 2022-06-17 15:57:17\n",
      "Encoded: 20000 @ 2022-06-17 16:15:10\n",
      "Encoded: 22000 @ 2022-06-17 16:32:58\n",
      "Encoded: 24000 @ 2022-06-17 16:50:28\n",
      "Encoded: 26000 @ 2022-06-17 17:08:48\n",
      "Encoded: 28000 @ 2022-06-17 17:28:32\n",
      "Encoded: 30000 @ 2022-06-17 17:47:26\n",
      "Encoded: 32000 @ 2022-06-17 18:07:29\n",
      "Encoded: 34000 @ 2022-06-17 18:27:13\n",
      "Encoded: 36000 @ 2022-06-17 18:46:49\n",
      "Encoded: 38000 @ 2022-06-17 19:06:17\n",
      "Encoded: 40000 @ 2022-06-17 19:27:14\n",
      "Encoded: 42000 @ 2022-06-17 19:48:03\n",
      "Encoded: 44000 @ 2022-06-17 20:09:24\n",
      "Encoded: 46000 @ 2022-06-17 20:30:42\n",
      "Encoded: 48000 @ 2022-06-17 20:51:27\n",
      "Encoded: 0 @ 2022-06-17 20:56:55\n",
      "Encoded: 2000 @ 2022-06-17 21:16:04\n",
      "Encoded: 4000 @ 2022-06-17 21:36:15\n",
      "Encoded: 6000 @ 2022-06-17 21:56:31\n",
      "Encoded: 8000 @ 2022-06-17 22:17:30\n",
      "Encoded: 10000 @ 2022-06-17 22:37:54\n",
      "Encoded: 12000 @ 2022-06-17 22:57:49\n",
      "Encoded: 14000 @ 2022-06-17 23:18:13\n",
      "Encoded: 16000 @ 2022-06-17 23:38:39\n",
      "Encoded: 18000 @ 2022-06-17 23:58:25\n",
      "Encoded: 20000 @ 2022-06-18 00:17:59\n",
      "Encoded: 22000 @ 2022-06-18 00:38:14\n",
      "Encoded: 24000 @ 2022-06-18 00:58:53\n",
      "Encoded: 26000 @ 2022-06-18 01:19:29\n",
      "Encoded: 28000 @ 2022-06-18 01:40:01\n",
      "Encoded: 30000 @ 2022-06-18 01:59:49\n",
      "Encoded: 32000 @ 2022-06-18 02:20:58\n"
     ]
    }
   ],
   "source": [
    "for f in all_task_files:\n",
    "    with open(f, 'r') as fin:\n",
    "        data_dict = json.load(fin)\n",
    "\n",
    "        out_file_name = f.rsplit(\"/\", 1)[-1]\n",
    "        out_file_name = out_file_name.replace(\"paper_metadata_\", \"\")\n",
    "        out_file_name = out_file_name.replace(\".json\", \"\")\n",
    "        \n",
    "        sd = de.encode_batch_wise_using_specter(data_dict, './data/shf-embeddings/{}.jsonl'.format(out_file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## trying to add titles to see if it makes sense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_task_files = glob.glob(\"./data/*.json\")\n",
    "\n",
    "# for f in all_task_files:\n",
    "#     with open(f, 'r') as fin:\n",
    "#         data_dict = json.load(fin)\n",
    "#         out_file_name = f.rsplit(\"/\", 1)[-1]\n",
    "#         out_file_name = out_file_name.replace(\"paper_metadata_\", \"\")\n",
    "#         out_file_name = out_file_name.replace(\".json\", \"\")\n",
    "        \n",
    "#         document_emb_dict = {}\n",
    "\n",
    "#         with open(\"./data/\" + out_file_name, 'r') as fi:\n",
    "#             for line in tqdm(fi, desc='reading embeddings from file...'):\n",
    "#             line_json = json.loads(line)\n",
    "#             document_emb_dict['paper_id']\n",
    "        \n",
    "#          with open(\"./data/Title_\" + out_file_name, 'w') as outfile:\n",
    "#             for _, entry in enumerate(document_emb_dict):\n",
    "#                 try:\n",
    "#                     outfile.write(json.dumps({\"paper_id\": entry, \"embedding\": document_emb_dict[entry]}) + '\\n')\n",
    "#                 except:\n",
    "#                     print(_, entry)\n",
    "#                     continue\n",
    "#         sd = de.encode_batch_wise_using_specter(data_dict, './data/shf-embeddings/{}.jsonl'.format(out_file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "eb05990e9cd43c92c320c03a0377d5005b41fd13f09f57c7ae9eebfb93a1b58f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
