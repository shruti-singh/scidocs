{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/singh_shruti/anaconda3/envs/oagbert/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# use sciemb conda env: conda activate sciemb\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from  cogdl.oag import oagbert\n",
    "import torch\n",
    "from datetime import datetime\n",
    "import json\n",
    "import glob "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataEncoder:\n",
    "\n",
    "    def __init__(self, model='specter_hf'):\n",
    "        # Init the tokenizer and encoding the model\n",
    "        if model == 'specter_hf':\n",
    "            self.load_specter_hf()\n",
    "        elif model == 'scibert':\n",
    "            self.load_scibert()\n",
    "        elif model == 'oagbert':\n",
    "            self.load_oagbert()\n",
    "        \n",
    "        # self.model.to('cuda:1')\n",
    "        self.model.eval()\n",
    "        return\n",
    "\n",
    "    def update_encoding_model(self, model_name):\n",
    "        if model_name == 'specter_hf':\n",
    "            self.load_specter()\n",
    "        elif model_name == 'scibert':\n",
    "            self.load_scibert()\n",
    "        elif model_name == 'oagbert':\n",
    "            self.load_oagbert()\n",
    "        else:\n",
    "            print('Invalid model name! Please try again with a valid  model name.')\n",
    "\n",
    "    def load_specter_hf(self):\n",
    "        self.model_name = 'specter_hf'\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained('allenai/specter')\n",
    "        self.model = AutoModel.from_pretrained('allenai/specter')\n",
    "        return\n",
    "\n",
    "    def load_scibert(self):\n",
    "        self.model_name = 'scibert'\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained('allenai/scibert_scivocab_cased')\n",
    "        self.model = AutoModel.from_pretrained('allenai/scibert_scivocab_cased')\n",
    "        return\n",
    "\n",
    "    def load_oagbert(self):\n",
    "        self.model_name = 'oagbert'\n",
    "        self.tokenizer, self.model = oagbert(\"oagbert-v2\")\n",
    "        return\n",
    "\n",
    "    def encode_batch_wise_using_oagbert(self, document_dict, to_save_loc):\n",
    "        document_emb_dict = {}\n",
    "\n",
    "        for _, d in enumerate(document_dict):\n",
    "            if _ % 2000 == 0:\n",
    "                print(\"Encoded: {} @ {}\".format(_, datetime.now().strftime('%Y-%m-%d %H:%M:%S')))\n",
    "            \n",
    "            # document_list is actually a dictionatry. org_pid_seq is not provided\n",
    "            paper_title = (document_dict[d].get('title') or '')\n",
    "            paper_abstract = (document_dict[d].get('abstract') or '')\n",
    "            document_id = d\n",
    "\n",
    "            #check if title+abs is empty\n",
    "            title_abs = paper_title + paper_abstract\n",
    "            if not title_abs.strip():\n",
    "                continue\n",
    "\n",
    "            input_ids, input_masks, token_type_ids, masked_lm_labels, position_ids, position_ids_second, \\\n",
    "            masked_positions, num_spans = self.model.build_inputs(\n",
    "                title=paper_title, abstract=paper_abstract, venue=[], authors=[], concepts=[], affiliations=[])\n",
    "            sequence_output, pooled_output = self.model.bert.forward(\n",
    "                input_ids=torch.LongTensor(input_ids).unsqueeze(0),\n",
    "                token_type_ids=torch.LongTensor(token_type_ids).unsqueeze(0),\n",
    "                attention_mask=torch.LongTensor(input_masks).unsqueeze(0),\n",
    "                output_all_encoded_layers=False,\n",
    "                checkpoint_activations=False,\n",
    "                position_ids=torch.LongTensor(position_ids).unsqueeze(0),\n",
    "                position_ids_second=torch.LongTensor(position_ids_second).unsqueeze(0)\n",
    "            )\n",
    "\n",
    "            embedding = pooled_output.detach().numpy().tolist()\n",
    "            document_emb_dict[document_id] = embedding\n",
    "\n",
    "            if _%4000 == 0:\n",
    "                print(\"Dumping 4k lines to file.\")\n",
    "                with open(to_save_loc, 'a') as outfile:\n",
    "                    for _, entry in enumerate(document_emb_dict):\n",
    "                        try:\n",
    "                            outfile.write(json.dumps({\"paper_id\": entry, \"title\": (document_dict[entry].get('title') or ''), \"embedding\": document_emb_dict[entry]}) + '\\n')\n",
    "                        except:\n",
    "                            print(_, entry)\n",
    "                            continue\n",
    "                document_emb_dict = {}\n",
    "\n",
    "        with open(to_save_loc, 'a') as outfile:\n",
    "            for _, entry in enumerate(document_emb_dict):\n",
    "                try:\n",
    "                    outfile.write(json.dumps({\"paper_id\": entry, \"title\": (document_dict[entry].get('title') or ''), \"embedding\": document_emb_dict[entry]}) + '\\n')\n",
    "                except:\n",
    "                    print(_, entry)\n",
    "                    continue\n",
    "\n",
    "        return document_emb_dict\n",
    "\n",
    "    def encode_batch_wise_using_specter(self, document_dict, to_save_loc, batch_size=20):\n",
    "        # List to contain small batch of douments and the corresponding doc ids\n",
    "        doc_batch_list = []\n",
    "        batch_ids = []\n",
    "\n",
    "        document_emb_dict = {}\n",
    "\n",
    "        for _, d in enumerate(document_dict):\n",
    "            if _ % 2000 == 0:\n",
    "                print(\"Encoded: {} @ {}\".format(_, datetime.now().strftime('%Y-%m-%d %H:%M:%S')))\n",
    "\n",
    "            doc_batch_list.append((document_dict[d].get('title') or '') + self.tokenizer.sep_token + (document_dict[d].get('abstract') or ''))\n",
    "            batch_ids.append(d)\n",
    "\n",
    "            if _%batch_size == 0:\n",
    "                inputs = self.tokenizer(doc_batch_list, padding=True, truncation=True, return_tensors=\"pt\", max_length=512)\n",
    "                # inputs = inputs.to('cuda:1')\n",
    "                result = self.model(**inputs)\n",
    "                embeddings = result.last_hidden_state[:, 0, :]\n",
    "\n",
    "                for ii, k in enumerate(batch_ids):\n",
    "                    document_emb_dict[k] = embeddings[ii].detach().cpu().numpy().tolist()\n",
    "\n",
    "                doc_batch_list = []\n",
    "                batch_ids = []\n",
    "\n",
    "        if batch_ids:\n",
    "            inputs = self.tokenizer(doc_batch_list, padding=True, truncation=True, return_tensors=\"pt\", max_length=512)\n",
    "            # inputs = inputs.to('cuda:1')\n",
    "            result = self.model(**inputs)\n",
    "            embeddings = result.last_hidden_state[:, 0, :]\n",
    "\n",
    "            for _, k in enumerate(batch_ids):\n",
    "                document_emb_dict[k] = embeddings[_].detach().cpu().numpy().tolist()\n",
    "\n",
    "        # To freeup memory in case of reuse in jupyter\n",
    "        doc_batch_list = []\n",
    "        batch_ids = []\n",
    "\n",
    "        with open(to_save_loc, 'w') as outfile:\n",
    "            for _, entry in enumerate(document_emb_dict):\n",
    "                try:\n",
    "                    outfile.write(json.dumps({\"paper_id\": entry, \"title\": (document_dict[entry].get('title') or ''), \"embedding\": document_emb_dict[entry]}) + '\\n')\n",
    "                except:\n",
    "                    print(_, entry)\n",
    "                    continue\n",
    "        return document_emb_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "file saved/oagbert-v2/config.json not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded: 0 @ 2022-06-17 13:18:57\n",
      "Encoded: 2000 @ 2022-06-17 17:11:42\n",
      "Encoded: 4000 @ 2022-06-17 20:14:49\n",
      "Encoded: 6000 @ 2022-06-17 23:24:12\n"
     ]
    }
   ],
   "source": [
    "de = DataEncoder(model='oagbert')\n",
    "\n",
    "all_task_files = glob.glob(\"./data/*.json\")\n",
    "for f in all_task_files:\n",
    "    with open(f, 'r') as fin:\n",
    "        data_dict = json.load(fin)\n",
    "\n",
    "        out_file_name = f.rsplit(\"/\", 1)[-1]\n",
    "        out_file_name = out_file_name.replace(\"paper_metadata_\", \"\")\n",
    "        out_file_name = out_file_name.replace(\".json\", \"\")\n",
    "        \n",
    "        sd = de.encode_batch_wise_using_oagbert(data_dict, './data/oag-embeddings/{}.jsonl'.format(out_file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode it again with the new oagbert embeddings in the oagbert conda environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "de = DataEncoder(model='oagbert')\n",
    "\n",
    "all_task_files = glob.glob(\"./data/*.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['./data/paper_metadata_mag_mesh.json',\n",
       "  './data/paper_metadata_recomm.json',\n",
       "  './data/paper_metadata_view_cite_read.json'],\n",
       " ['./data/paper_metadata_view_cite_read.json'])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_task_files, all_task_files[-1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to encode!\n",
      "Encoded: 0 @ 2022-06-25 13:41:24\n",
      "Dumping 4k lines to file.\n",
      "Encoded: 2000 @ 2022-06-25 14:10:26\n",
      "Encoded: 4000 @ 2022-06-25 14:29:39\n",
      "Dumping 4k lines to file.\n",
      "Encoded: 6000 @ 2022-06-25 14:35:16\n",
      "Encoded: 8000 @ 2022-06-25 14:40:49\n",
      "Dumping 4k lines to file.\n",
      "Encoded: 10000 @ 2022-06-25 14:46:13\n",
      "Encoded: 12000 @ 2022-06-25 14:51:30\n",
      "Dumping 4k lines to file.\n",
      "Encoded: 14000 @ 2022-06-25 14:57:15\n",
      "Encoded: 16000 @ 2022-06-25 15:02:59\n",
      "Dumping 4k lines to file.\n",
      "Encoded: 18000 @ 2022-06-25 15:08:36\n",
      "Encoded: 20000 @ 2022-06-25 15:14:26\n",
      "Dumping 4k lines to file.\n",
      "Encoded: 22000 @ 2022-06-25 15:19:58\n",
      "Encoded: 24000 @ 2022-06-25 15:25:28\n",
      "Dumping 4k lines to file.\n",
      "Encoded: 26000 @ 2022-06-25 15:31:14\n",
      "Encoded: 28000 @ 2022-06-25 15:36:47\n",
      "Dumping 4k lines to file.\n",
      "Encoded: 30000 @ 2022-06-25 15:42:16\n",
      "Encoded: 32000 @ 2022-06-25 15:47:32\n",
      "Dumping 4k lines to file.\n",
      "Encoded: 34000 @ 2022-06-25 16:16:08\n",
      "Encoded: 36000 @ 2022-06-25 16:33:39\n",
      "Dumping 4k lines to file.\n",
      "Encoded: 38000 @ 2022-06-25 17:12:19\n",
      "Encoded: 40000 @ 2022-06-25 17:32:35\n",
      "Dumping 4k lines to file.\n",
      "Encoded: 42000 @ 2022-06-25 17:38:03\n",
      "Encoded: 44000 @ 2022-06-25 17:43:36\n",
      "Dumping 4k lines to file.\n",
      "Encoded: 46000 @ 2022-06-25 17:49:31\n",
      "Encoded: 48000 @ 2022-06-25 17:55:10\n",
      "Dumping 4k lines to file.\n",
      "Encoded: 50000 @ 2022-06-25 18:00:30\n",
      "Encoded: 52000 @ 2022-06-25 18:06:08\n",
      "Dumping 4k lines to file.\n",
      "Encoded: 54000 @ 2022-06-25 18:36:18\n",
      "Encoded: 56000 @ 2022-06-25 18:54:29\n",
      "Dumping 4k lines to file.\n",
      "Encoded: 58000 @ 2022-06-25 19:27:33\n",
      "Encoded: 60000 @ 2022-06-25 19:49:29\n",
      "Dumping 4k lines to file.\n",
      "Encoded: 62000 @ 2022-06-25 19:54:55\n",
      "Encoded: 64000 @ 2022-06-25 20:31:22\n",
      "Dumping 4k lines to file.\n",
      "Encoded: 66000 @ 2022-06-25 20:56:13\n",
      "Encoded: 68000 @ 2022-06-25 21:13:50\n",
      "Dumping 4k lines to file.\n",
      "Encoded: 70000 @ 2022-06-25 21:38:13\n",
      "Encoded: 72000 @ 2022-06-25 22:08:27\n",
      "Dumping 4k lines to file.\n",
      "Encoded: 74000 @ 2022-06-25 22:42:46\n",
      "Encoded: 76000 @ 2022-06-25 23:14:49\n",
      "Dumping 4k lines to file.\n",
      "Encoded: 78000 @ 2022-06-25 23:20:38\n",
      "Encoded: 80000 @ 2022-06-25 23:26:11\n",
      "Dumping 4k lines to file.\n",
      "Encoded: 82000 @ 2022-06-25 23:31:46\n",
      "Encoded: 84000 @ 2022-06-25 23:37:16\n",
      "Dumping 4k lines to file.\n",
      "Encoded: 86000 @ 2022-06-25 23:42:47\n",
      "Encoded: 88000 @ 2022-06-25 23:48:20\n",
      "Dumping 4k lines to file.\n",
      "Encoded: 90000 @ 2022-06-25 23:53:50\n",
      "Encoded: 92000 @ 2022-06-25 23:59:23\n",
      "Dumping 4k lines to file.\n",
      "Encoded: 94000 @ 2022-06-26 00:05:07\n",
      "Encoded: 96000 @ 2022-06-26 00:10:54\n",
      "Dumping 4k lines to file.\n",
      "Encoded: 98000 @ 2022-06-26 00:16:46\n",
      "Encoded: 100000 @ 2022-06-26 00:22:48\n",
      "Dumping 4k lines to file.\n",
      "Encoded: 102000 @ 2022-06-26 00:28:48\n",
      "Encoded: 104000 @ 2022-06-26 00:34:51\n",
      "Dumping 4k lines to file.\n",
      "Encoded: 106000 @ 2022-06-26 00:40:52\n",
      "Encoded: 108000 @ 2022-06-26 00:46:58\n",
      "Dumping 4k lines to file.\n",
      "Encoded: 110000 @ 2022-06-26 00:53:04\n",
      "Encoded: 112000 @ 2022-06-26 00:59:10\n",
      "Dumping 4k lines to file.\n",
      "Encoded: 114000 @ 2022-06-26 01:05:08\n",
      "Encoded: 116000 @ 2022-06-26 01:11:12\n",
      "Dumping 4k lines to file.\n",
      "Encoded: 118000 @ 2022-06-26 01:17:08\n",
      "Encoded: 120000 @ 2022-06-26 01:23:11\n",
      "Dumping 4k lines to file.\n",
      "Encoded: 122000 @ 2022-06-26 01:29:38\n",
      "Encoded: 124000 @ 2022-06-26 01:36:06\n",
      "Dumping 4k lines to file.\n",
      "Encoded: 126000 @ 2022-06-26 01:41:53\n",
      "Encoded: 128000 @ 2022-06-26 02:07:25\n",
      "Dumping 4k lines to file.\n",
      "Encoded: 130000 @ 2022-06-26 02:20:25\n",
      "Encoded: 132000 @ 2022-06-26 02:26:52\n",
      "Dumping 4k lines to file.\n",
      "Encoded: 134000 @ 2022-06-26 02:38:35\n",
      "Encoded: 136000 @ 2022-06-26 02:50:30\n",
      "Dumping 4k lines to file.\n",
      "Encoded: 138000 @ 2022-06-26 03:00:22\n",
      "Encoded: 140000 @ 2022-06-26 03:15:11\n",
      "Dumping 4k lines to file.\n",
      "Encoded: 142000 @ 2022-06-26 03:29:39\n"
     ]
    }
   ],
   "source": [
    "for f in all_task_files[-1:]:\n",
    "    with open(f, 'r') as fin:\n",
    "        data_dict = json.load(fin)\n",
    "\n",
    "        out_file_name = f.rsplit(\"/\", 1)[-1]\n",
    "        out_file_name = out_file_name.replace(\"paper_metadata_\", \"\")\n",
    "        out_file_name = out_file_name.replace(\".json\", \"\")\n",
    "        \n",
    "        print(\"Starting to encode!\")\n",
    "        \n",
    "        sd = de.encode_batch_wise_using_oagbert(data_dict, './data/trained_embs/newoag-embeddings/{}.jsonl'.format(out_file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oagbert",
   "language": "python",
   "name": "oagbert"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "eb05990e9cd43c92c320c03a0377d5005b41fd13f09f57c7ae9eebfb93a1b58f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
